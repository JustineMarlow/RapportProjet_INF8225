%pour la mise en page
\documentclass[a4paper,12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}

%pour les figures
\usepackage{float}
\usepackage{graphicx}

%pour les maths
\usepackage{amsmath}
\usepackage{amsfonts}

%pour les hyperliens
\usepackage{hyperref}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}

\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}

\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand\headrulewidth{1pt}
\fancyhead[L]{Rapport de projet - INF8225}
\fancyhead[R]{}
\renewcommand\footrulewidth{1pt}
\fancyfoot[L]{}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{}

\makeatletter
\let\ps@plain=\ps@fancy
\renewcommand{\thesection}{\@arabic\c@section.}
\renewcommand{\thesubsection}{\@arabic\c@section.\@arabic\c@subsection}
\renewcommand{\bibname}{Bibliographie}
\renewcommand{\contentsname}{Table des matières}
\makeatother

\begin{document}
\begin{titlepage}

  \begin{center}
  \vspace*{4cm}
    \rule{\linewidth}{0.5pt}
    \bigbreak
    {\Huge {INF8225} }
    \bigbreak
    {\LARGE {Rapport de projet}}
    \medbreak
    \rule{\linewidth}{.5pt}
    \bigbreak
    Michel Fabrice \textsc{Serret}\\
    Justine \textsc{Marlow}\\
    Brieuc \textsc{Dandin}\\
    Nicolas \textsc{Valenchon}\\
    \vfill
	\includegraphics[scale=1]{Logo_Polytechnique_Montréal_(partenariat_Wikimédia).png}
	\vfill

\end{center}
\end{titlepage}

\tableofcontents

\newpage \section*{Introduction}
\addcontentsline{toc}{section}{Introduction}
L'objectif de notre projet est d'observer les effets de la combinaison de deux méthodes en apprentissage par renforcement :
\begin{itemize}
    \item l'imitation dans l'apprentissage
    \item l'utilisation de la curiosité dans l'apprentissage
\end{itemize}
En outre, nous souhaitons déterminer si l'utilisation conjointe ou séquentielle de ces deux méthodes permet d'améliorer les performances du modèle au delà des deux méthodes séparées.\\

Parmi les méthodes de curiosité existantes aujourd'hui en \textit{machine learning}, nous nous sommes intéressés aux méthodes :
\begin{itemize}
    \item \textit{Curiosity-driven Exploration by Self-supervised Prediction} \cite{curiosity} \footnote{Nous avons réalisé notre présentation en INF8225 sur cet article de recherche}
    \item \textit{Exploration by Random Network Distillation} \cite{distillation}
    \item les méthodes utilisées par les agents de \textit{Never Give Up} \cite{ngu} et de l'agent 57 \cite{agent57}
\end{itemize}
Par ailleurs, nous nous sommes intéressé à la méthode d'imitation \textit{SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards} \cite{sqil}.\\

Nous présenterons donc dans ce rapport une revue de littérature de ces différentes méthodes, avant de présenter l'expérience que nous avons tenté de mener.\\

Le code developpé dans le cadre de ce projet est disponble sur un \textit{repository Github} à l'adresse suivante \url{https://github.com/fserret/Projet_INF8225}.

\newpage \section{Revue : \textit{Exploration by Random Network Distillation} \cite{distillation}}
\label{revue_distillation}
\indent La méthode de \textit{random network distillation} vis à résoudre deux problèmes communs des réseaux de \textit{reinforcement learning} :
\newline Le premier est que leur entraînement est complexe lorsque la répartition des récompenses n'est pas dense.
\newline Une solution est d’utiliser les erreurs de prédiction pour quantifier les nouvelles expériences et ainsi corriger le système par une récompense intrinsèque. On peut inciter l’agent à aller vers des états non ou peu explorés. Une solution complémentaire consiste à énumérer tous les états visités et à augmenter la récompense intrinsèque pour les états les moins visités. Les récompenses extrinsèques et intrinsèques pouvant être à des échelles différentes, il est par ailleurs proposé dans l'article de normaliser les récompenses intrinsèques.
\newline \indent  Le deuxième problème que les Burda, Edwards \& al. cherchent à résoudre grâce à la RDN est le phénomène de procrastination : si l’agent peut influencer une action dont le résultat est aléatoire (jouer à pile ou face par exemple), il ne va pas aller explorer ailleurs.
\newline Pour cela, il faudrait quantifier l’amélioration relative de la prédiction, \textit{id est} prédire si l’agent apprend quelque chose en restant dans cet état. Avec une mesure de cet apprentissage, l'agent peut ainsi éviter de procrastiner : ne pouvant apprendre d'un évènement aléatoire, il continuerait à explorer. Une telle mesure serait en revanche difficile à implémenter.
\newline
\newline \indent  Le RDN utilise deux réseaux. Le premier, le \textit{target network}, extrait des observations de l’agent une version plus représentative. Le deuxième est entraîné afin d’imiter le premier en minimisant l’erreur quadratique entre les deux.
\newline
\newline \indent  Entraîner l’agent de manière épisodique peut être contre-productif : même si l’agent perd pour avoir essayé d’explorer une zone nouvelle et qu’il n’a eu aucune récompense pour cela, il n'essayera pas d'y retourner lors d'un nouvel épisode d'exploration, aucune récompense n'étant associée à cette zone. Inversement, si l'agent continue d'explorer jusqu'à finalement obtenir une grande récompense en explorant la zone lui ayant pourtant fourni une récompense nulle durant le même épisode, alors obtenir plusieurs échecs successifs pour arriver à cette récompense peut devenir une solution rentable.
\newline En cela, l'agent mimique ainsi le comportement d'un joueur humain : il ne s'arrête pas de jouer lorsqu’il perd ; mais il recommence, surtout s’il sait qu’il progresse dans une zone difficile dont la récompense est à la hauteur de la difficulté.
\newline Il faut tout de même réaliser des épisodes, car l’agent peut se retrouver bloqué dans une boucle infinie dès le début du test à condition de pouvoir facilement obtenir une récompense à nouveau disponible.

\newpage \section{Revue : Agent \textit{Never Give Up} \cite{ngu} et agent 57 \cite{agent57}}
\subsection{NGU}
\indent Un état déjà visité est considéré par les algorithmes curiosité comme non pertinent, alors qu’il pourrait encore en réalité permettre à l’agent d’apprendre.
\newline \indent  NGU dispose d’une récompense intrinsèque qui va inciter l’agent à visiter les états connus pendant un épisode (mais une fois seulement), pour chercher à être de plus en plus familier avec eux. Plusieurs facteurs impactent sur cette récompense : un facteur de nouveauté épisodique, qui s’adapte rapidement aux nouvelles observations du réseau, et un facteur longue durée, qui se base sur un RND et qui varie plus lentement. En d’autres termes, cette récompense puni rapidement la visite d’un état connu au sein d’un même épisode, elle décourage lentement la visite d’état fréquemment visité et elle cherche à ignorer les observations de l’environnement qui ne peuvent pas être influencées par les actions de l’agent.
\newline \indent  Pour extraire les éléments utiles des observations de l’agent, un réseau Siamois est utilisé. Le module va chercher à prédire l’action qui permet de passer d’un état x(t) à un état x(t+1). Les deux états passent en parallèle dans le réseau Siamois, qui va en ressortir deux représentations d’états (une pour chaque). Et en partant de ces représentation, un classifieur est utilisé afin de prédire l’action qui permet de passer d’un état à un autre. Une rétropropagation permet de faire évoluer les deux réseaux (siamois + classifieur) afin de minimiser l’erreur entre l’action prévu par le classifieur et l’action réel, qui a permis de passer de l'état x(t) à l'état x(t+1).
\newline \indent  La mémoire épisodique va contenir tous les états qui sont visités pendant l’épisode. La récompense intrinsèque sera donnée par une fonction inverse du nombre de fois que l’état, ou un voisin très proche de l’état a été visité. La récompense va donc diminuer pour les états déjà visité pendant cet épisode.

\subsection{Agent 57}
\indent Objectif : réussir à battre le niveau moyen humain des 57 jeux Atari. Problème : tous les jeux ne se nécessite pas la même stratégie. Certains on besoin de récompense long terme, d’autre de beaucoup d’exploration. Il faut être capable de réussir tous les jeux à la fois.
\newline \indent  L’agent se base sur NGU mais y apporte des améliorations. La fonction Q de mesure de la qualité d’une action est séparée en deux, pour optimiser les composants intrinsèques et extrinsèques séparément. Le choix de l’action la plus optimisée est tout de même fait en utilisant ces deux fonctions.
\newline \indent  Plusieurs politiques sont entraînés en parallèle, mais contrairement à d’autre agent, celui-ci peut prioriser certaines politique, qui permettent un apprentissage plus efficace.
\newline
\newline \indent  Parler de la séparation entre actor et learner


\newpage \section{Revue : \textit{Soft Q imitation learning (SQIL): Imitation Learning via Reinforcement Learning with Sparse Rewards} \cite{sqil}}
\label{revue_SQIL}
Il peut être très complexe d'entraîner un agent à imiter un expert sur la base de démonstrations, et ce, particulièrement à travers l'observation continue d'un environnement aux nombreuses dimensions et comprenant des dynamiques inconnues. Les méthodes d'apprentissage supervisé se basant sur le \textit{behavioral cloning} (BC) souffrent de dérive de la distribution - \textit{distribution shifting}. En effet, en imitant trop les actions démontrées, l'agent accumule des erreurs et dérive des états recherchés.
\newline \indent Des approches récentes basées sur l'apprentissage par renforcement - \textit{reinforcement learning} (RL) -, dont l'\textit{inverse RL} et le \textit{generative adversarial imitation learning} (GAIL), résolvent ce problème. Mais cela est au prix de l'entraînement d'un agent RL qui pour coller aux démonstrations sur le long terme. La véritable fonction de récompense étant inconnue, cela implique que son apprentissage se base sur les démonstrations, au prix de techniques d'approximation hasardeuses et complexes qui utilisent un entraînement adversariel.
\newline \indent Reddy \& al. proposent une alternative qui, bien qu'utilisant de l'apprentissage par renforcement, ne nécessite pas d'apprendre une fonction de récompense. Leur méthode emploie un agent qui retourne aux états déjà démontrés lorsqu'il en rencontre de nouveaux, afin de coïncider avec les démonstrations sur le long terme. Afin de revenir à ces états régulièrement, l'agent reçoit une récompense constante $r=+1$ lorsqu'il effectue l'action démontrée pour un état démontré, et de $r=0$ autrement.
\newline \indent Les auteurs de l'article présentant SQIL le comparent aux méthodes de BC sur une variété de tâches en petite dimension et basées sur des images avec Box2D, Atari et MuJoCo. Les résultats de SQIL sont supérieurs à ceux obtenus avec des méthodes de \textit{behavioral cloning}, et placent cette méthode au même niveau que GAIL.
\newline \indent SQIL est une preuve de concept que même de simples méthodes d'imitation basée sur l'apprentissage par renforcement avec des récompenses constantes peuvent être aussi efficaces que des méthodes bien plus complexes, avec lesquelles les récompenses sont apprises. En théorie, SQIL peut toutefois être interprété comme une variante régularisée de \textit{behavioral cloning} qui utilise de la dispersion - \textit{sparcity} - avant d'encourager l'imitation sur le long-terme.
\newline \indent Cette méthode peut être implémentée avec n'importe quel algorithme d'acteur-critique sans politique ou de \textit{Q-learning} standard avec très peu de modifications, raison pour laquelle elle s'avère particulièrement intéressante dans le cas de notre projet.
\newline \indent Pourtant, ses auteurs n'ont pas pu prouver que SQIL atteint les mêmes performances en termes d'occupation des états dans la limite de démonstrations finies. De plus, SQIL ne trouve que la politique de l'expert, mais pas sa fonction de récompense.

\newpage \section{Expérience}
\subsection{Méthode de curiosité choisie}
Nous avons choisi d'utiliser la \textit{random network distillation} en utilisant le code fourni avec l'article de recherche (disponible sur un \textit{repository Github} \cite{distillation_github}).

\subsection{Méthode d'imitation choisie}
Notre objectif était d'utiliser la méthode d'apprentissage par renforcement par imitation \textit{SQIL} (cf. partie \ref{revue_SQIL} et article \cite{sqil}).\\
\indent Cependant, dû à un manque de temps et le fait de n'avoir trouvé aucun \textit{repository} utilisable, nous avons décidé de définir une récompense intrinsèque guidée par les démonstrations d'un agent expert (ici, notre agent entraîné avec la curiosité de \textit{random network distillation}).\\
\indent Pour ce faire nous avons utilisé une méthode similaire au \textit{pseudo-count} de la mémoire épisodique de l'agent NGU mais pour comparer les observations de notre agent avec celles des démonstrations de notre agent expert. Par manque de temps, et comme nous nous intéressons ici à ce que les états soit similaire nous avons utilisée directement les produit scalaire entre les vecteurs de \textit{features} crées par le \textit{random target network} du modèle en cours d'entraînement comme \textit{kernel} de similarité, ensuite nous combinons le \textit{reward} ainsi obtenu avec le \textit{reward} de curiosité comme suit:
\begin{equation} r_{im} = \frac{\sum_{v\in \text{Demonstrations}} X_{obs}\cdot v}{|\text{Demonstrations}|} \end{equation}

\begin{equation} r_{int} = r_{rnd} \cdot (1 + tanh(\alpha \cdot r_{im})) \end{equation}

\subsection{Combinaison de l'imitation et de la curiosité}
Nous avons donc choisi de comparer les performances de notre agent avec curiosité (\textit{random network distillation}) avec notre agent imitateur de l'agent \textit{expert} (le premier).

\subsection{Problème dans l'implémentation de l'agent avec imitation}
\indent Notre implémentation du \textit{reward} $r_{im}$, définie ci-dessus, souffre d'une erreur dont nous n'avons malheureusement pas trouvé la source et qui nous empêche d'aller au-delà de 400 étapes par épisode durant l'entraînement.
En effet, l'apprentissage avec ce \textit{reward} fonctionne jusqu'à ce que la mémoire de l'ordinateur soit remplie et que les processus MPI et python associés soient \textit{killed}.
Malgré la diminution du nombre de processus concurrent à 8 (initialement 32) et du nombre d'observations de l'agent expert à 54 (initialement 864), nous n'avons pas pu aller au-delà d'épisodes de 400 étapes pour l'entraînement avec ce \textit{reward}. Cette erreur est probablement dûe à un \textit{memory-leak} dans notre code, cependant l'hypothèse du manque de mémoire n'a pas été écartée (par manque de moyen).

\subsection{Environnement de test}
%Pour tester notre modèle, nous avons utilisé la librairie \textit{python} \textit{gym} \cite{gym}, qui permet de tester et comparer des modèles en apprentissage par renforcement, notamment sur des jeux vidéos.\\
Le code fourni pour la \textit{random network distillation} offre différents environnements de test qui sont des jeux vidéos d'\textit{Atari}, nous avons choisi d'entraîner et de tester nos agents sur le jeu \textit{Montezuma's revenge}\cite{montezuma}. Dans ce jeu, l'objectif du joueur est d'explorer un ensemble de salles en évitant des ennemis et des pièges (qui entraînent la mort du joueur).

\subsection{Résultats}
\subsubsection{Paramètres observables pendant l'apprentissage}
On peut s'intéresser à différents paramètres que l'on peut observer au fil de l'apprentissage de l'agent à savoir :
\begin{itemize}
    \item \textit{n\_rooms} : le nombre de salles atteintes par l'agent depuis le début de l'apprentissage
    \item \textit{rewtotal} : la somme des récompenses obtenues par l'agent depuis le début de l'apprentissage
    \item \textit{best\_ret} : la valeur de récompense la plus importante déjà rencontrée par l'agent depuis le début de l'apprentissage
    \item \textit{eplen} : la longueur des épisodes d'apprentissage menés par l'agent
    \item \textit{eprooms} : le nombre de salles atteintes par l'agent en un épisode d'apprentissage
\end{itemize}

\subsubsection{Graphiques des données obtenues lors de l'apprentissage}
\indent Dans le but de comparer les deux modèles d'apprentissage, nous avons préparé un code de mise en forme des résultats en graphiques (en utilisant notamment la librairie \textit{python} \textit{matplotlib}) afin de pouvoir comparer nos deux agents sur les différents paramètres énoncés plus haut.\\

\indent Étant dans l'impossibilité de mener l'apprentissage de l'agent par imitation suffisamment loin pour obtenir des résultats intéressants, nous présenterons en annexe 1 (à partir de la page \pageref{annexe_1}) les graphiques d'apprentissage de l'agent avec curiosité et en annexe 2 (à partir de la page \pageref{annexe_2}) la superposition des courbes d'apprentissage de l'agent avec curiosité et l'agent avec imitation sur les 400 premières étapes d'apprentissage (seules données à notre disposition pour l'agent avec imitation).\\

\indent Les graphiques présentés dans l'annexe 1 montrent que notre agent avec curiosité parvient bien à \textit{apprendre} au fil du temps : \textit{n\_rooms}, \textit{rewtotal} et \textit{best\_ret} augmentent. A l'échelle des épisodes, on constate que globalement \textit{eplen} augmente (ce qui indique que l'agent est capable de joueur au jeu de plus en plus longtemps, sans \textit{game-over}) et \textit{eprooms} augmente globalement également. On constate tout de même une régression de \textit{eplen} et \textit{eprooms} autour de la 3000ème étape d'apprentissage, ce qui pourrait indiquer un sur-apprentissage. Les graphiques présentés dans l'annexe 2 ne nous permettent malheureusement pas de tirer des conclusions.

\newpage
\subsection*{Annexe 1 - Graphiques d'apprentissage de l'agent avec curiosité}
\addcontentsline{toc}{section}{Annexe 1}
\label{annexe_1}
    \centering
    \includegraphics[width=\textwidth]{curiosity_only/curiosity_n_rooms.JPG}
    \includegraphics[width=\textwidth]{curiosity_only/curiosity_rewtotal.JPG}
    \includegraphics[width=\textwidth]{curiosity_only/curiosity_best_ret.JPG}
    \includegraphics[width=\textwidth]{curiosity_only/curiosity_eplen.JPG}
    \includegraphics[width=\textwidth]{curiosity_only/curiosity_eprooms.JPG}

\newpage
\subsection*{Annexe 2 - Graphiques d'apprentissage de l'agent avec curiosité et de l'agent avec imitation}
\addcontentsline{toc}{section}{Annexe 2}
\label{annexe_2}
    \centering
    \includegraphics[width=\textwidth]{curiosity_imitation/curiosity_imitation_n_rooms.JPG}
    \includegraphics[width=\textwidth]{curiosity_imitation/curiosity_imitation_rewtotal.JPG}
    \includegraphics[width=\textwidth]{curiosity_imitation/curiosity_imitation_best_ret.JPG}
    \includegraphics[width=\textwidth]{curiosity_imitation/curiosity_imitation_eplen.JPG}
    \includegraphics[width=\textwidth]{curiosity_imitation/curiosity_imitation_eprooms.JPG}

\begin{thebibliography}{9}
\addcontentsline{toc}{section}{Bibliographie}
    \bibitem{curiosity}
	  Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell,\\
	  \textit{Curiosity-driven Exploration by Self-supervised Prediction},\\
	  Mai 2017,\\
	  \url{https://arxiv.org/abs/1705.05363}

    \bibitem{distillation}
	 Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov,\\
	  \textit{Exploration by Random Network Distillation},\\
	  Octobre 2016,\\
	  \url{https://arxiv.org/abs/1810.12894}

    \bibitem{ngu}
	 Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martín Arjovsky, Alexander Pritzel, Andew Bolt, Charles Blundell,\\
	  \textit{Never Give Up: Learning Directed Exploration Strategies},\\
	  Février 2020,\\
	  \url{https://arxiv.org/abs/2002.06038}

    \bibitem{agent57}
	 Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Charles Blundell,\\
	  \textit{Agent57: Outperforming the Atari Human Benchmark},\\
	  Mars 2020,\\
	  \url{https://arxiv.org/abs/2003.13350}

    \bibitem{distillation_github}
	  \textit{Github repo for random network distillation}\\
	  \url{https://github.com/openai/random-network-distillation}

		\bibitem{sqil}
	  Siddharth Reddy, Anca D. Dragan, Sergey Levine,\\
	  \textit{SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards},\\
	  Septembre 2019,\\
	  \url{https://arxiv.org/abs/1905.11108}

	\bibitem{montezuma}
	Jeu vidéo \textit{Montezuma's revenge}\\
	\url{https://en.wikipedia.org/wiki/Montezuma%27s_Revenge_(video_game)}
\end{thebibliography}

\end{document}
