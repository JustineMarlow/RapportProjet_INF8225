%pour la mise en page
\documentclass[a4paper,12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}

%pour les figures
\usepackage{float}
\usepackage{graphicx}

%pour les maths
\usepackage{amsmath}
\usepackage{amsfonts}

%pour les hyperliens
\usepackage{hyperref}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}

\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}

\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand\headrulewidth{1pt}
\fancyhead[L]{Rapport de projet - INF8225}
\fancyhead[R]{}
\renewcommand\footrulewidth{1pt}
\fancyfoot[L]{}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{}

\makeatletter
\let\ps@plain=\ps@fancy
\renewcommand{\thesection}{\@arabic\c@section.}
\renewcommand{\thesubsection}{\@arabic\c@section.\@arabic\c@subsection}
\renewcommand{\bibname}{Bibliographie}
\makeatother

\begin{document}
\begin{titlepage}

  \begin{center}
  \vspace*{4cm}
    \rule{\linewidth}{0.5pt}
    \bigbreak
    {\Huge {INF8225} }
    \bigbreak
    {\LARGE {Rapport de projet}}
    \medbreak
    \rule{\linewidth}{.5pt}
    \bigbreak
    Michel Fabrice \textsc{Serret}\\
    Justine \textsc{Marlow}\\
    Brieuc \textsc{Dandin}\\
    Nicolas \textsc{Valenchon}\\
    \vfill
	\includegraphics[scale=1]{Logo_Polytechnique_Montréal_(partenariat_Wikimédia).png}
	\vfill

\end{center}
\end{titlepage}

\section*{Introduction}
L'objectif de notre projet est d'observer les effets de la combinaison de deux méthodes en apprentissage par renforcement :
\begin{itemize}
    \item l'imitation dans l'apprentissage
    \item l'utilisation de la curiosité dans l'apprentissage
\end{itemize}
En outre, nous souhaitons déterminer si l'utilisation conjointe ou séquentielle de ces deux méthodes permet d'améliorer les performances du modèle au delà des deux méthodes séparées.\\

Parmi les méthodes de curiosité existantes aujourd'hui en \textit{machine learning}, nous nous sommes intéressés aux méthodes :
\begin{itemize}
    \item \textit{Curiosity-driven Exploration by Self-supervised Prediction} \cite{curiosity} \footnote{Nous avons réalisé notre présentation en INF8225 sur cet article de recherche}
    \item \textit{Exploration by Random Network Distillation} \cite{distillation}
    \item les méthodes utilisées par les agents de \textit{Never Give Up} \cite{ngu} et de l'agent 57 \cite{agent57}
\end{itemize}
Par ailleurs, nous nous sommes intéressé à la méthode d'imitation \textit{SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards} \cite{sqil}.\\

Nous présenterons donc dans ce rapport une revue de littérature de ces différentes méthodes, avant de présenter l'expérience que nous avons tenté de mener.

\newpage \section{Revue : \textit{Exploration by Random Network Distillation} \cite{distillation}}
\label{revue_distillation}

\newpage \section{Revue : \textit{Soft Q imitation learning (SQIL): Imitation Learning via Reinforcement Learning with Sparse Rewards} \cite{sqil}}
\label{revue_SQIL}
Il peut être très complexe d'entraîner un agent à imiter un expert sur la base de démonstrations, et ce, particulièrement à travers l'observation continue d'un environnement aux nombreuses dimensions et comprenant des dynamiques inconnues. Les méthodes d'apprentissage supervisé se basant sur le \textit{behavioral cloning} (BC) souffrent de dérive de la distribution - \textit{distribution shifting}. En effet, en imitant trop les actions démontrées, l'agent accumule des erreurs et dérive des états recherchés.
\newline Des approches récentes basées sur l'apprentissage par renforcement - \textit{reinforcement learning} (RL) -, dont l'\textit{inverse RL} et le \textit{generative adversarial imitation learning} (GAIL), résolvent ce problème. Mais cela est au prix de l'entraînement d'un agent RL qui pour coller aux démonstrations sur le long terme. La véritable fonction de récompense étant inconnue, cela implique que son apprentissage se base sur les démonstrations, au prix de techniques d'approximation hasardeuses et complexes qui utilise un entraînement adversariel.
\newline
\newline \indent Reddy \& al. proposent une alternative qui, bien qu'utilisant de l'apprentissage par renforcement, ne nécessite pas d'apprendre une fonction de récompense. Leur méthode emploie un agent qui retourne aux états déjà démontrés lorsqu'il en rencontre de nouveaux, afin de coïncider avec les démonstrations sur le long terme. Afin de revenir à ces états régulièrement, l'agent reçoit une récompense constante $r=+1$ lorsqu'il effectue l'action démontrée pour un état démontrée, et de $r=0$ autrement.
\newline
\newline \indent Les auteurs de l'article présentant SQIL le comparent aux méthodes de BC sur une variété de tâches en petite dimension et basées sur des images avec Box2D, Atari et MuJoCo. Les résultats de SQIL sont supérieurs à ceux obtenus avec des méthodes de \textit{behavioral cloning}, et placent cette méthode au même niveau que GAIL.
\newline
\newline \indent SQIL est une preuve de concept que même de simple méthodes d'imitation basée sur l'apprentissage par renforcement avec des récompenses constantes peut être aussi efficace que des méthodes bien plus complexes, avec lesquelles les récompenses sont apprises. En théorie, SQIL peut toutefois être interprété comme une variante régularisée de \textit{behavioral cloning} qui utilise de la dispersion - \textit{sparcity} - avant d'encourager l'imitation sur le long-terme.
\newline
\newline \indent Cette méthode peut être implémentée avec n'importe quel algorithme d'acteur-critique sans politique ou de \textit{Q-learning} standard avec très peu de modifications, raison pour laquelle elle s'avère particulièrement intéressante dans le cas de notre projet.

\newpage \section{Expérience}
\subsection{Méthode de curiosité choisie}
Nous avons choisi d'utiliser la \textit{random network distillation} en utilisant le code fourni avec l'article de recherche (disponible sur un \textit{repository Github} \cite{distillation_github}).

\subsection{Méthode d'imitation choisie}
Notre objectif était d'utiliser la méthode d'apprentissage par renforcement par imitation \textit{SQIL} (cf. partie \ref{revue_SQIL} et article \cite{sqil}).\\
\indent Cependant, dû à un manque de temps et le fait de n'avoir trouvé aucun \textit{repository} utilisable, nous avons décidé de définir une récompense intrinsèque guidée par les démonstrations d'un agent expert (ici, notre agent entraîné avec la curiosité de \textit{random network distillation}).\\
\indent Pour ce faire nous avons utilisé une méthode similaire au \textit{pseudo-count} de la mémoire épisodique de l'agent NGU mais pour comparer les observations de notre agent avec celles des démonstrations de notre agent expert. Par manque de temps, et comme nous nous intéressons ici à ce que les états soit similaire nous avons utilisée directement les produit scalaire entre les vecteurs de \textit{features} crées par le \textit{random target network} du modèle en cours d'entraînement comme \textit{kernel} de similarité, ensuite nous combinons le \textit{reward} ainsi obtenu avec le \textit{reward} de curiosité comme suit:
\begin{equation} r_{im} = \frac{\sum_{v\in \text{Demonstrations}} X_{obs}\cdot v}{|\text{Demonstrations}|} \end{equation}

\begin{equation} r_{int} = r_{rnd} \cdot (1 + tanh(\alpha \cdot r_{im})) \end{equation}

\subsection{Combinaison de l'imitation et de la curiosité}
Nous avons donc choisi de comparer les performances de notre agent avec curiosité (\textit{random network distillation}) avec notre agent imitateur de l'agent \textit{expert} (le premier).

\subsection{Problème dans l'implémentation de l'agent avec imitation}
\indent Notre implémentation du \textit{reward} $r_{im}$, définie ci-dessus, souffre d'une erreur dont nous n'avons malheureusement pas trouvé la source et qui nous empêche d'aller au-delà de 400 étapes par épisode durant l'entraînement.
En effet, l'apprentissage avec ce \textit{reward} fonctionne jusqu'à ce que la mémoire de l'ordinateur soit remplie et que les processus MPI et python associés soient \textit{killed}.
Malgré la diminution du nombre de processus concurrent à 8 (initialement 32) et du nombre d'observations de l'agent expert à 54 (initialement 864), nous n'avons pas pu aller au-delà d'épisodes de 400 étapes pour l'entraînement avec ce \textit{reward}. Cette erreur est probablement dûe à un \textit{memory-leak} dans notre code, cependant l'hypothèse du manque de mémoire n'a pas été écartée (par manque de moyen).

\subsection{Environnement de test}
%Pour tester notre modèle, nous avons utilisé la librairie \textit{python} \textit{gym} \cite{gym}, qui permet de tester et comparer des modèles en apprentissage par renforcement, notamment sur des jeux vidéos.\\
Le code fourni pour la \textit{random network distillation} offre différents environnements de test qui sont des jeux vidéos d'\textit{Atari}, nous avons choisi d'entraîner et de tester nos agents sur le jeu \textit{Montezuma's revenge}\cite{montezuma}. Dans ce jeu, l'objectif du joueur est d'explorer un ensemble de salles en évitant des ennemis et des pièges (qui entraînent la mort du joueur).

\subsection{Résultats}
\subsubsection{Paramètres observables pendant l'apprentissage}
On peut s'intéresser à différents paramètres que l'on peut observer au fil de l'apprentissage de l'agent à savoir :
\begin{itemize}
    \item \textit{n\_rooms} : le nombre de salles atteintes par l'agent depuis le début de l'apprentissage
    \item \textit{rewtotal} : la somme des récompenses obtenues par l'agent depuis le début de l'apprentissage
    \item \textit{best\_ret} : la valeur de récompense la plus importante déjà rencontrée par l'agent depuis le début de l'apprentissage
    \item \textit{eplen} : la longueur des épisodes d'apprentissage menés par l'agent
    \item \textit{eprooms} : le nombre de salles atteintes par l'agent en un épisode d'apprentissage
\end{itemize}

\subsubsection{Graphiques des données obtenues lors de l'apprentissage}
\indent Dans le but de comparer les deux modèles d'apprentissage, nous avons préparé un code de mise en forme des résultats en graphiques (en utilisant notamment la librairie \textit{python} \textit{matplotlib}) afin de pouvoir comparer nos deux agents sur les différents paramètres énoncés plus haut.\\

\indent Étant dans l'impossibilité de mener l'apprentissage de l'agent par imitation suffisamment loin pour obtenir des résultats intéressants, nous présenterons en annexe 1 (à partir de la page \pageref{annexe_1}) les graphiques d'apprentissage de l'agent avec curiosité et en annexe 2 (à partir de la page \pageref{annexe_2}) la superposition des courbes d'apprentissage de l'agent avec curiosité et l'agent avec imitation sur les 400 premières étapes d'apprentissage (seules données à notre disposition pour l'agent avec imitation).\\

\indent Les graphiques présentés dans l'annexe 1 montrent que notre agent avec curiosité parvient bien à \textit{apprendre} au fil du temps : \textit{n\_rooms}, \textit{rewtotal} et \textit{best\_ret} augmentent. A l'échelle des épisodes, on constate que globalement \textit{eplen} augmente (ce qui indique que l'agent est capable de joueur au jeu de plus en plus longtemps, sans \textit{game-over}) et \textit{eprooms} augmente globalement également. On constate tout de même une régression de \textit{eplen} et \textit{eprooms} autour de la 3000ème étape d'apprentissage, ce qui pourrait indiquer un sur-apprentissage. Les graphiques présentés dans l'annexe 2 ne nous permettent malheureusement pas de tirer des conclusions.

\newpage
\subsection*{Annexe 1 - Graphiques d'apprentissage de l'agent avec curiosité}
\label{annexe_1}
    \centering
    \includegraphics[width=\textwidth]{curiosity_only/curiosity_n_rooms.JPG}
    \includegraphics[width=\textwidth]{curiosity_only/curiosity_rewtotal.JPG}
    \includegraphics[width=\textwidth]{curiosity_only/curiosity_best_ret.JPG}
    \includegraphics[width=\textwidth]{curiosity_only/curiosity_eplen.JPG}
    \includegraphics[width=\textwidth]{curiosity_only/curiosity_eprooms.JPG}

\newpage
\subsection*{Annexe 2 - Graphiques d'apprentissage de l'agent avec curiosité et de l'agent avec imitation}
\label{annexe_2}
    \centering
    \includegraphics[width=\textwidth]{curiosity_imitation/curiosity_imitation_n_rooms.JPG}
    \includegraphics[width=\textwidth]{curiosity_imitation/curiosity_imitation_rewtotal.JPG}
    \includegraphics[width=\textwidth]{curiosity_imitation/curiosity_imitation_best_ret.JPG}
    \includegraphics[width=\textwidth]{curiosity_imitation/curiosity_imitation_eplen.JPG}
    \includegraphics[width=\textwidth]{curiosity_imitation/curiosity_imitation_eprooms.JPG}

\begin{thebibliography}{9}
    \bibitem{curiosity}
	  Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell,\\
	  \textit{Curiosity-driven Exploration by Self-supervised Prediction},\\
	  Mai 2017,\\
	  \url{https://arxiv.org/abs/1705.05363}

    \bibitem{distillation}
	 Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov,\\
	  \textit{Exploration by Random Network Distillation},\\
	  Octobre 2016,\\
	  \url{https://arxiv.org/abs/1810.12894}

    \bibitem{ngu}
	 Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martín Arjovsky, Alexander Pritzel, Andew Bolt, Charles Blundell,\\
	  \textit{Never Give Up: Learning Directed Exploration Strategies},\\
	  Février 2020,\\
	  \url{https://arxiv.org/abs/2002.06038}

    \bibitem{agent57}
	 Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Charles Blundell,\\
	  \textit{Agent57: Outperforming the Atari Human Benchmark},\\
	  Mars 2020,\\
	  \url{https://arxiv.org/abs/2003.13350}

    \bibitem{distillation_github}
	  \textit{Github repo for random network distillation}\\
	  \url{https://github.com/openai/random-network-distillation}

		\bibitem{sqil}
	  Siddharth Reddy, Anca D. Dragan, Sergey Levine,\\
	  \textit{SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards},\\
	  Septembre 2019,\\
	  \url{https://arxiv.org/abs/1905.11108}

	\bibitem{montezuma}
	Jeu vidéo \textit{Montezuma's revenge}\\
	\url{https://en.wikipedia.org/wiki/Montezuma%27s_Revenge_(video_game)}
\end{thebibliography}

\end{document}
